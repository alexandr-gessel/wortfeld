<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Über das Projekt – Wortfeld</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
</head>
<body class="bg-gray-100 text-gray-800">
  <main class="w-full max-w-screen-md mx-auto px-4 py-10 leading-relaxed text-lg">

    <div class="text-left mb-6">
      <a href="/" class="inline-flex items-center text-blue-600 hover:underline text-sm">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" viewBox="0 0 20 20" fill="currentColor">
          <path fill-rule="evenodd" d="M7.707 14.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 111.414 1.414L3.414 8H17a1 1 0 110 2H3.414l3.293 3.293a1 1 0 01-1.414 1.414z" clip-rule="evenodd"></path>
        </svg>
        Zurück zur Übersicht
      </a>
    </div>

    <h1 class="text-3xl font-bold mb-8 text-center">Über das Projekt t </h1>

    <section class="space-y-6">
      <p>
        Dieses Projekt entstand aus praktischer Erfahrung mit der Entwicklung eines Systems zur Beobachtung deutschsprachiger Nachrichtenquellen in einem Analysezentrum (2020–2021), wo ich als Backend-Entwickler tätig war. Damals bestand meine Aufgabe darin, den Prozess der Erhebung, Verarbeitung und Analyse von Nachrichten für Marketingkampagnen von Agrar-Exporteuren aus der GUS zu automatisieren.
      </p>
      <p>
        Ich entwickelte individuelle Parser für führende Nachrichtenportale (mittels <code>requests</code>, <code>aiohttp</code>, <code>BeautifulSoup</code>, <code>lxml</code>), organisierte die Datenspeicherung in MongoDB und PostgreSQL und integrierte ein <code>spaCy</code>-Modell zur Extraktion von Entitäten, Schlüsselbegriffen und Stimmungen. Zudem wurde ein E-Mail-Warnsystem eingerichtet, das die Stabilität der Parser überwachte. Diese Lösung ermöglichte eine vollständig automatisierte Medienbeobachtung und verbesserte die Qualität der Marketinganalysen erheblich.
      </p>
      <p>
        Später, während meiner Arbeit im Team von <a href="https://dryshaft.net" class="text-blue-600 hover:underline" target="_blank">DryShaft Data Lab</a> zur <a href="https://pythia.one/bundeswahl.html" class="text-blue-600 hover:underline" target="_blank">Bundestagswahl 2025</a>, war ich erneut mit ähnlichen Aufgaben betraut — insbesondere mit dem Sammeln und Auswerten von deutschsprachigen Presseartikeln und Expertenquellen. Der Quellcode und die Ergebnisse dieses Projekts unterliegen dem geistigen Eigentum des Unternehmens, doch die eingesetzten Methoden und Architekturen habe ich in diesem offenen Projekt adaptiert und weiterentwickelt.
      </p>
      <p>
        Ziel des Projekts Wortfeld war es, einen einfachen, aber funktionierenden Prototyp für die Analyse deutschsprachiger Nachrichtenartikel am Beispiel von Tagesschau zu erstellen. Das Projekt dient als technisches Demonstrationsbeispiel für mein Portfolio als Python-Backend-Entwickler mit Schwerpunkt auf Textverarbeitung, FastAPI und MongoDB.
      </p>

      <h2 class="text-2xl font-semibold mt-8 mb-2">Was im MVP umgesetzt wurde</h2>
      <p><strong>Korpus:</strong> Aus über 11.000 Artikeln auf Tagesschau.de wurden 96 Artikel manuell ausgewählt — etwa 8 bis 10 pro Monat. Diese Auswahl deckt rund 10 % des Gesamtbestands ab und gewährleistet Übersichtlichkeit und Vielfalt, ohne das System zu überladen.</p>
      <p><strong>Datenstrukturierung:</strong> Alle Texte wurden bereinigt, normalisiert und in MongoDB gespeichert. Verwendete Felder sind: <code>title</code>, <code>date</code>, <code>atlas</code>, <code>topics</code>, <code>text</code>, <code>entities</code>, <code>noun_chunks</code>, <code>lemmas</code>, <code>tfidf_keywords</code>, <code>google_entities</code>.</p>
      <p><strong>NLP-Analyse (basierend auf spaCy):</strong> Tokenisierung, Lemmatisierung, Extraktion benannter Entitäten und Nominalgruppen, POS-Filterung, Entfernung von Stoppwörtern und Duplikaten.</p>
      <p><strong>TF-IDF:</strong> Schlüsselbegriffe wurden aus einer TF-IDF-Matrix extrahiert und als farbige Schlagwörter visualisiert (Themen, Geografie, neue Begriffe).</p>
      <p><strong>Google Natural Language API:</strong> Entitäten mit Typ (PERSON, LOCATION, ORGANIZATION), Salienz und ggf. Wikipedia-Link. Die Kategorien funktionieren auf Deutsch kaum, daher war die Nutzung dort ineffektiv. Die Entitäten hingegen waren nützlich für Analyse und Visualisierung.</p>

      <h2 class="text-2xl font-semibold mt-8 mb-2">Benutzeroberfläche</h2>
      <ul class="list-disc list-inside">
        <li>Startseite: Übersicht mit Titel, Datum, Quelle und Schlagwörtern</li>
        <li>Artikelseite: Volltext mit Analyseblöcken für Tags und Google NLP</li>
        <li>Infoblock „Über dieses Projekt“ am Ende jeder Artikelseite</li>
      </ul>

      <h2 class="text-2xl font-semibold mt-8 mb-2">Fazit</h2>
      <p>
        Wortfeld ist ein einfacher Prototyp, in dem ich den vollständigen Pipeline-Zyklus umgesetzt habe — von der Datenerhebung und Analyse bis zur Darstellung in einem Webinterface. Das Projekt zeigt, wie man einen Nachrichtenanalyse-Workflow für deutschsprachige Texte mit Bibliotheken wie spaCy, scikit-learn, FastAPI, Jinja2, MongoDB und externen Services wie Google NLP aufbauen kann.
      </p>
      <p>
        Trotz vereinfachtem Design und teils manuell kuratierter Artikelauswahl veranschaulicht das Projekt meine Herangehensweise an Datenstrukturierung, Analysepriorisierung und Textverarbeitung unter realen Bedingungen.
      </p>
    </section>
  </main>
</body>
</html>
